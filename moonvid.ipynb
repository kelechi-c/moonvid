{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os, torch\n",
    "from PIL import Image as pillow\n",
    "from typing import Union\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_path = \"moondream2\"\n",
    "local_tokenizer_path = \"moondream2_tokenizer\"\n",
    "llama_model_path = \"llama_3-2_1B\"\n",
    "\n",
    "model_id = \"vikhyatk/moondream2\"\n",
    "llama_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "revision = \"2024-08-26\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "video_path = \"/kaggle/input/vid-sample/jujutsu-kaisen-shibuya-arc-uraume-shibuya-arc.mp4\"\n",
    "output_folder = \"vidframes\"\n",
    "sample_rate = 1  # Extract a frame every 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(\n",
    "    video_path: Union[str, os.PathLike],\n",
    "    output_folder: str = output_folder,\n",
    "    sample_rate: int = 1\n",
    ") -> os.PathLike:\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Create output folder if it doesn't exist\n",
    "\n",
    "    video = cv2.VideoCapture(video_path) # read video file with cv2\n",
    "\n",
    "    # get video properties\n",
    "    fps = video.get(cv2.CAP_PROP_FPS) # frames per second in the video\n",
    "    num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT)) # total number of frames\n",
    "    frame_interval = int(fps * sample_rate) # Calculate the frame interval based on the sample rate\n",
    "\n",
    "    # Initialize frame counter\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        success, frame = video.read()  # Read a frame\n",
    "        # Extract frame at specified intervals\n",
    "        if frame_count % frame_interval == 0 and success:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{frame_count:06d}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame) # write frame to jpeg/image file\n",
    "        else:\n",
    "            break # stop when theres an error in frame extraction \n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    video.release()\n",
    "\n",
    "    print(f\"Extracted {frame_count // frame_interval} of {num_frames} total frames, image frames saved at {output_folder}\")\n",
    "\n",
    "    return output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = extract_frames(video_path, output_folder, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(\n",
    "    md_model_path: Union[str, os.PathLike] = local_model_path,\n",
    "    md_tokenizer_path: Union[str, os.PathLike] = local_tokenizer_path,\n",
    "    llama_path: str = llama_model_path,\n",
    "    model_id: str = model_id,\n",
    "    llama_id: str = llama_model_id\n",
    ") -> tuple:\n",
    "    md_model = None\n",
    "    md_tokenizer = None\n",
    "    llama_pipe = None\n",
    "\n",
    "    is_local = os.path.isdir(md_model_path)  # check if previously saved models are available\n",
    "    llm_is_local = os.path.isdir(llama_model_path)\n",
    "\n",
    "    if is_local and llm_is_local:  # load from locally saved weights\n",
    "        print('loading from local checkpoint')\n",
    "        md_model = AutoModelForCausalLM.from_pretrained(md_model_path)\n",
    "        md_tokenizer = AutoTokenizer.from_pretrained(md_tokenizer_path)\n",
    "        llama_pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=llama_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "    else:  # download fresh weights from huggingface\n",
    "        print('downloading weights from huggingface')\n",
    "        md_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        md_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        llama_pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=llama_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        # then save locally for next time\n",
    "        llama_pipe.save_pretrained(llama_model_path)\n",
    "        md_model.save_pretrained(md_model_path)\n",
    "        md_tokenizer.save_pretrained(md_tokenizer_path) # type: ignore\n",
    "\n",
    "    return md_model, md_tokenizer, llama_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moondream_model, md_tokenizer, llama_pipe = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidframes = [os.path.join(output_folder, path) for path in os.listdir(output_folder)]\n",
    "\n",
    "image_frames = [pillow.open(img) for img in vidframes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_frames(\n",
    "    image_frames: list,\n",
    "    model: AutoModelForCausalLM = moondream_model,\n",
    "    tokenizer: AutoTokenizer = md_tokenizer\n",
    ") -> list:\n",
    "    \n",
    "    captions = []\n",
    "\n",
    "    for frame in image_frames:\n",
    "        enc_image = model.encode_image(frame) # encode image with vision encoder(moondream uses SigLip)\n",
    "        frame_caption = model.answer_question(enc_image, \"briefly describe this image\", tokenizer) # generate caption\n",
    "        \n",
    "        captions.append(frame_caption)\n",
    "        \n",
    "    return captions\n",
    "\n",
    "captions = caption_frames(image_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_captions(captions: list, llm_pipeline: pipeline = llama_pipe) -> str:\n",
    "    single_cap = '.'.join(captions) \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a summary chatbot who summarizes and merges several image captions into one long sentence\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"{single_cap}\"},\n",
    "    ]\n",
    "\n",
    "    outputs = llm_pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    llm_caption = outputs[0][\"generated_text\"][-1]\n",
    "    \n",
    "    return llm_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
